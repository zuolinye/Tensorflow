{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "损失函数",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/zuolinye/Tensorflow/blob/master/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "XQ09cUfy_7HW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. 激活函数（activation function）"
      ]
    },
    {
      "metadata": {
        "id": "ksJGIYsOAN5j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "有效避免纯线性组合，提高了模型的表达力，使模型具有更好的区分度"
      ]
    },
    {
      "metadata": {
        "id": "viE0mBOFAjX0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. 偏置项bias"
      ]
    },
    {
      "metadata": {
        "id": "QNhdKU-XAGvA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3. NN复杂度：多用NN层数和NN参数的个数表示\n",
        "层数 = 隐藏层的层数 + 1个输出层\n",
        "\n",
        "总参数 = 总W + 总b"
      ]
    },
    {
      "metadata": {
        "id": "NlfbXwsqBbEU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "优化：损失函数loss，学习率learning_rate，滑动平均ema，正则化regularization"
      ]
    },
    {
      "metadata": {
        "id": "yZs1eXh0BvmO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "kyRVLAOR_0bq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2951
        },
        "outputId": "670336f9-3cf4-448d-e67f-6fff10255f4d"
      },
      "cell_type": "code",
      "source": [
        "#coding:utf-8\n",
        "#预测多或预测少的影响一样\n",
        "#0导入模块，生成数据集\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "BATCH_SIZE = 8\n",
        "SEED = 23455\n",
        "\n",
        "rdm = np.random.RandomState(SEED)\n",
        "X = rdm.rand(32,2)\n",
        "Y_ = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
        "\n",
        "#1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
        "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
        "y = tf.matmul(x, w1)\n",
        "\n",
        "#2定义损失函数及反向传播方法。\n",
        "#定义损失函数为MSE,反向传播方法为梯度下降。\n",
        "loss_mse = tf.reduce_mean(tf.square(y_ - y))\n",
        "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss_mse)\n",
        "\n",
        "#3生成会话，训练STEPS轮\n",
        "with tf.Session() as sess:\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    sess.run(init_op)\n",
        "    STEPS = 20000\n",
        "    for i in range(STEPS):\n",
        "        start = (i*BATCH_SIZE) % 32\n",
        "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
        "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y_[start:end]})\n",
        "        if i % 500 == 0:\n",
        "            print \"After %d training steps, w1 is: \" % (i)\n",
        "            print sess.run(w1), \"\\n\"\n",
        "    print \"Final w1 is: \\n\", sess.run(w1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After 0 training steps, w1 is: \n",
            "[[-0.80974597]\n",
            " [ 1.4852903 ]] \n",
            "\n",
            "After 500 training steps, w1 is: \n",
            "[[-0.46074435]\n",
            " [ 1.641878  ]] \n",
            "\n",
            "After 1000 training steps, w1 is: \n",
            "[[-0.21939856]\n",
            " [ 1.6984766 ]] \n",
            "\n",
            "After 1500 training steps, w1 is: \n",
            "[[-0.04415595]\n",
            " [ 1.7003176 ]] \n",
            "\n",
            "After 2000 training steps, w1 is: \n",
            "[[0.08942621]\n",
            " [1.673328  ]] \n",
            "\n",
            "After 2500 training steps, w1 is: \n",
            "[[0.19583553]\n",
            " [1.6322677 ]] \n",
            "\n",
            "After 3000 training steps, w1 is: \n",
            "[[0.28375748]\n",
            " [1.5854434 ]] \n",
            "\n",
            "After 3500 training steps, w1 is: \n",
            "[[0.35848638]\n",
            " [1.5374471 ]] \n",
            "\n",
            "After 4000 training steps, w1 is: \n",
            "[[0.4233252]\n",
            " [1.4907392]] \n",
            "\n",
            "After 4500 training steps, w1 is: \n",
            "[[0.48040032]\n",
            " [1.4465573 ]] \n",
            "\n",
            "After 5000 training steps, w1 is: \n",
            "[[0.5311361]\n",
            " [1.4054534]] \n",
            "\n",
            "After 5500 training steps, w1 is: \n",
            "[[0.57653254]\n",
            " [1.367594  ]] \n",
            "\n",
            "After 6000 training steps, w1 is: \n",
            "[[0.6173259]\n",
            " [1.3329402]] \n",
            "\n",
            "After 6500 training steps, w1 is: \n",
            "[[0.65408474]\n",
            " [1.3013425 ]] \n",
            "\n",
            "After 7000 training steps, w1 is: \n",
            "[[0.68726856]\n",
            " [1.2726018 ]] \n",
            "\n",
            "After 7500 training steps, w1 is: \n",
            "[[0.7172598]\n",
            " [1.2465004]] \n",
            "\n",
            "After 8000 training steps, w1 is: \n",
            "[[0.74438614]\n",
            " [1.2228196 ]] \n",
            "\n",
            "After 8500 training steps, w1 is: \n",
            "[[0.7689325]\n",
            " [1.2013482]] \n",
            "\n",
            "After 9000 training steps, w1 is: \n",
            "[[0.79115146]\n",
            " [1.1818888 ]] \n",
            "\n",
            "After 9500 training steps, w1 is: \n",
            "[[0.81126714]\n",
            " [1.1642567 ]] \n",
            "\n",
            "After 10000 training steps, w1 is: \n",
            "[[0.8294814]\n",
            " [1.1482829]] \n",
            "\n",
            "After 10500 training steps, w1 is: \n",
            "[[0.84597576]\n",
            " [1.1338127 ]] \n",
            "\n",
            "After 11000 training steps, w1 is: \n",
            "[[0.8609128]\n",
            " [1.1207061]] \n",
            "\n",
            "After 11500 training steps, w1 is: \n",
            "[[0.87444043]\n",
            " [1.1088346 ]] \n",
            "\n",
            "After 12000 training steps, w1 is: \n",
            "[[0.88669145]\n",
            " [1.0980824 ]] \n",
            "\n",
            "After 12500 training steps, w1 is: \n",
            "[[0.8977863]\n",
            " [1.0883439]] \n",
            "\n",
            "After 13000 training steps, w1 is: \n",
            "[[0.9078348]\n",
            " [1.0795243]] \n",
            "\n",
            "After 13500 training steps, w1 is: \n",
            "[[0.91693527]\n",
            " [1.0715363 ]] \n",
            "\n",
            "After 14000 training steps, w1 is: \n",
            "[[0.92517716]\n",
            " [1.0643018 ]] \n",
            "\n",
            "After 14500 training steps, w1 is: \n",
            "[[0.93264157]\n",
            " [1.0577497 ]] \n",
            "\n",
            "After 15000 training steps, w1 is: \n",
            "[[0.9394023]\n",
            " [1.0518153]] \n",
            "\n",
            "After 15500 training steps, w1 is: \n",
            "[[0.9455251]\n",
            " [1.0464406]] \n",
            "\n",
            "After 16000 training steps, w1 is: \n",
            "[[0.95107025]\n",
            " [1.0415728 ]] \n",
            "\n",
            "After 16500 training steps, w1 is: \n",
            "[[0.9560928]\n",
            " [1.037164 ]] \n",
            "\n",
            "After 17000 training steps, w1 is: \n",
            "[[0.96064115]\n",
            " [1.0331714 ]] \n",
            "\n",
            "After 17500 training steps, w1 is: \n",
            "[[0.96476096]\n",
            " [1.0295546 ]] \n",
            "\n",
            "After 18000 training steps, w1 is: \n",
            "[[0.9684917]\n",
            " [1.0262802]] \n",
            "\n",
            "After 18500 training steps, w1 is: \n",
            "[[0.9718707]\n",
            " [1.0233142]] \n",
            "\n",
            "After 19000 training steps, w1 is: \n",
            "[[0.974931 ]\n",
            " [1.0206276]] \n",
            "\n",
            "After 19500 training steps, w1 is: \n",
            "[[0.9777026]\n",
            " [1.0181949]] \n",
            "\n",
            "Final w1 is: \n",
            "[[0.98019385]\n",
            " [1.0159807 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L3lfAYSuDIbx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "212b93ac-657a-4982-9d79-95c119f53785"
      },
      "cell_type": "code",
      "source": [
        "#coding:utf-8\n",
        "#酸奶成本1元， 酸奶利润9元\n",
        "#预测少了损失大，故不要预测少，故生成的模型会多预测一些\n",
        "#0导入模块，生成数据集\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "BATCH_SIZE = 8\n",
        "SEED = 23455\n",
        "COST = 1\n",
        "PROFIT = 9\n",
        "\n",
        "rdm = np.random.RandomState(SEED)\n",
        "X = rdm.rand(32,2)\n",
        "Y = [[x1+x2+(rdm.rand()/10.0-0.05)] for (x1, x2) in X]\n",
        "\n",
        "#1定义神经网络的输入、参数和输出，定义前向传播过程。\n",
        "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
        "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
        "w1= tf.Variable(tf.random_normal([2, 1], stddev=1, seed=1))\n",
        "y = tf.matmul(x, w1)\n",
        "\n",
        "#2定义损失函数及反向传播方法。\n",
        "# 定义损失函数使得预测少了的损失大，于是模型应该偏向多的方向预测。\n",
        "loss = tf.reduce_sum(tf.where(tf.greater(y, y_), (y - y_)*COST, (y_ - y)*PROFIT))\n",
        "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
        "\n",
        "#3生成会话，训练STEPS轮。\n",
        "with tf.Session() as sess:\n",
        "    init_op = tf.global_variables_initializer()\n",
        "    sess.run(init_op)\n",
        "    STEPS = 3000\n",
        "    for i in range(STEPS):\n",
        "        start = (i*BATCH_SIZE) % 32\n",
        "        end = (i*BATCH_SIZE) % 32 + BATCH_SIZE\n",
        "        sess.run(train_step, feed_dict={x: X[start:end], y_: Y[start:end]})\n",
        "        if i % 500 == 0:\n",
        "            print \"After %d training steps, w1 is: \" % (i)\n",
        "            print sess.run(w1), \"\\n\"\n",
        "    print \"Final w1 is: \\n\", sess.run(w1)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After 0 training steps, w1 is: \n",
            "[[-0.762993 ]\n",
            " [ 1.5095658]] \n",
            "\n",
            "After 500 training steps, w1 is: \n",
            "[[1.0235443]\n",
            " [1.0463371]] \n",
            "\n",
            "After 1000 training steps, w1 is: \n",
            "[[1.0174844]\n",
            " [1.0406414]] \n",
            "\n",
            "After 1500 training steps, w1 is: \n",
            "[[1.0211805]\n",
            " [1.0472372]] \n",
            "\n",
            "After 2000 training steps, w1 is: \n",
            "[[1.0179386]\n",
            " [1.041272 ]] \n",
            "\n",
            "After 2500 training steps, w1 is: \n",
            "[[1.0205938]\n",
            " [1.0390443]] \n",
            "\n",
            "Final w1 is: \n",
            "[[1.0296593]\n",
            " [1.0484141]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9jWKL-TDF3cI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}